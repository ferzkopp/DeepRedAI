{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "001066fc",
   "metadata": {},
   "source": [
    "# Test Local Embedding\n",
    "\n",
    "Notebook that tests the local LMStudio embedding model accessibility and functionality.\n",
    "\n",
    "Evalates the API performance for different parameter and batching configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef28a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# LM Studio server endpoint\n",
    "HOST = \"192.168.X.Y\"  # Replace with your server's IP address\n",
    "LM_STUDIO_URL = f\"http://{HOST}:1234\"\n",
    "\n",
    "# Get the list of models\n",
    "response = requests.get(f\"{LM_STUDIO_URL}/v1/models\")\n",
    "response.raise_for_status()\n",
    "\n",
    "models = response.json()\n",
    "\n",
    "print(\"Available models:\")\n",
    "for model in models.get(\"data\", []):\n",
    "    print(f\"  - {model['id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9393ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import concurrent.futures\n",
    "from typing import List\n",
    "\n",
    "# Embedding model to test (note: corrected typo from \"nomix\" to \"nomic\")\n",
    "EMBEDDING_MODEL = \"text-embedding-nomic-embed-text-v1.5@f16\"\n",
    "\n",
    "def get_embeddings(texts: List[str], model: str = EMBEDDING_MODEL) -> dict:\n",
    "    \"\"\"Get embeddings for a list of texts using the batch API.\"\"\"\n",
    "    response = requests.post(\n",
    "        f\"{LM_STUDIO_URL}/v1/embeddings\",\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"input\": texts  # LM Studio supports batched input\n",
    "        }\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def get_embedding_single(text: str, model: str = EMBEDDING_MODEL) -> dict:\n",
    "    \"\"\"Get embedding for a single text.\"\"\"\n",
    "    response = requests.post(\n",
    "        f\"{LM_STUDIO_URL}/v1/embeddings\",\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"input\": text\n",
    "        }\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "# Sample texts for testing\n",
    "test_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Python is a popular programming language for data science.\",\n",
    "    \"LM Studio allows running local language models.\",\n",
    "    \"Embeddings convert text into numerical vectors.\",\n",
    "    \"Vector databases store and search embeddings efficiently.\",\n",
    "    \"Neural networks learn patterns from training data.\",\n",
    "    \"Natural language processing enables computers to understand text.\",\n",
    "    \"Transformers revolutionized the field of NLP.\",\n",
    "    \"Local AI provides privacy and control over your data.\"\n",
    "]\n",
    "\n",
    "print(f\"Testing embedding model: {EMBEDDING_MODEL}\")\n",
    "print(f\"Number of test texts: {len(test_texts)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test 1: Single requests (sequential)\n",
    "print(\"\\n1. Sequential single requests:\")\n",
    "start_time = time.perf_counter()\n",
    "for text in test_texts:\n",
    "    result = get_embedding_single(text)\n",
    "sequential_time = time.perf_counter() - start_time\n",
    "print(f\"   Total time: {sequential_time:.3f}s\")\n",
    "print(f\"   Avg per text: {sequential_time/len(test_texts)*1000:.1f}ms\")\n",
    "\n",
    "# Test 2: Batched request (all texts at once)\n",
    "print(\"\\n2. Batched request (all texts at once):\")\n",
    "start_time = time.perf_counter()\n",
    "result = get_embeddings(test_texts)\n",
    "batch_time = time.perf_counter() - start_time\n",
    "print(f\"   Total time: {batch_time:.3f}s\")\n",
    "print(f\"   Avg per text: {batch_time/len(test_texts)*1000:.1f}ms\")\n",
    "print(f\"   Embedding dimension: {len(result['data'][0]['embedding'])}\")\n",
    "\n",
    "# Test 3: Parallel requests using ThreadPoolExecutor\n",
    "print(\"\\n3. Parallel single requests (4 workers):\")\n",
    "start_time = time.perf_counter()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    futures = [executor.submit(get_embedding_single, text) for text in test_texts]\n",
    "    results = [f.result() for f in concurrent.futures.as_completed(futures)]\n",
    "parallel_time = time.perf_counter() - start_time\n",
    "print(f\"   Total time: {parallel_time:.3f}s\")\n",
    "print(f\"   Avg per text: {parallel_time/len(test_texts)*1000:.1f}ms\")\n",
    "\n",
    "# Test 4: Larger batch performance test\n",
    "print(\"\\n4. Throughput test (100 texts in batches of 10):\")\n",
    "large_test = test_texts * 10  # 100 texts\n",
    "batch_size = 10\n",
    "start_time = time.perf_counter()\n",
    "for i in range(0, len(large_test), batch_size):\n",
    "    batch = large_test[i:i+batch_size]\n",
    "    get_embeddings(batch)\n",
    "large_batch_time = time.perf_counter() - start_time\n",
    "print(f\"   Total time: {large_batch_time:.3f}s\")\n",
    "print(f\"   Throughput: {len(large_test)/large_batch_time:.1f} texts/second\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Sequential:  {sequential_time:.3f}s (baseline)\")\n",
    "print(f\"Batched:     {batch_time:.3f}s ({sequential_time/batch_time:.1f}x faster)\")\n",
    "print(f\"Parallel:    {parallel_time:.3f}s ({sequential_time/parallel_time:.1f}x faster)\")\n",
    "print(f\"\\nRecommendation: Use batched requests for best performance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f71bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Iterator, Tuple\n",
    "\n",
    "@dataclass\n",
    "class EmbeddingResult:\n",
    "    \"\"\"Result of an embedding operation.\"\"\"\n",
    "    text: str\n",
    "    embedding: List[float]\n",
    "    index: int\n",
    "    \n",
    "    @property\n",
    "    def vector(self) -> np.ndarray:\n",
    "        \"\"\"Return embedding as numpy array.\"\"\"\n",
    "        return np.array(self.embedding, dtype=np.float32)\n",
    "\n",
    "class EmbeddingClient:\n",
    "    \"\"\"\n",
    "    Client for generating text embeddings using LM Studio's API.\n",
    "    Optimized for batch processing to populate vector databases.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        base_url: str = \"http://localhost:1234\",\n",
    "        model: str = \"text-embedding-nomic-embed-text-v1.5@f16\",\n",
    "        batch_size: int = 32,\n",
    "        timeout: int = 60,\n",
    "        # Model configuration limits\n",
    "        context_length: int = 2048,\n",
    "        model_batch_size: int = 1024\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the embedding client.\n",
    "        \n",
    "        Args:\n",
    "            base_url: LM Studio server URL\n",
    "            model: Embedding model identifier\n",
    "            batch_size: Number of texts to process per API call\n",
    "            timeout: Request timeout in seconds\n",
    "            context_length: Maximum context length in tokens (model config)\n",
    "            model_batch_size: Maximum batch size in tokens (model config)\n",
    "        \"\"\"\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.timeout = timeout\n",
    "        self.context_length = context_length\n",
    "        self.model_batch_size = model_batch_size\n",
    "        self._dimension: Optional[int] = None\n",
    "    \n",
    "    @property\n",
    "    def dimension(self) -> int:\n",
    "        \"\"\"Get the embedding dimension (lazy-loaded).\"\"\"\n",
    "        if self._dimension is None:\n",
    "            # Get dimension by embedding a test string\n",
    "            result = self._embed_batch([\"test\"])\n",
    "            self._dimension = len(result[0])\n",
    "        return self._dimension\n",
    "    \n",
    "    def estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Estimate token count for a text (rough approximation).\n",
    "        Uses ~4 chars per token as a conservative estimate.\n",
    "        \"\"\"\n",
    "        return len(text) // 4 + 1\n",
    "    \n",
    "    def max_safe_text_length(self, num_texts: int = 1) -> int:\n",
    "        \"\"\"\n",
    "        Calculate maximum safe text length in characters for a batch.\n",
    "        \n",
    "        Args:\n",
    "            num_texts: Number of texts in the batch\n",
    "            \n",
    "        Returns:\n",
    "            Maximum characters per text to stay within limits\n",
    "        \"\"\"\n",
    "        # Use the more restrictive limit\n",
    "        tokens_per_text = min(\n",
    "            self.context_length // num_texts,\n",
    "            self.model_batch_size // num_texts\n",
    "        )\n",
    "        # Conservative: 3 chars per token to leave headroom\n",
    "        return max(tokens_per_text * 3, 50)\n",
    "    \n",
    "    def _embed_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Internal method to embed a batch of texts.\"\"\"\n",
    "        response = requests.post(\n",
    "            f\"{self.base_url}/v1/embeddings\",\n",
    "            json={\"model\": self.model, \"input\": texts},\n",
    "            timeout=self.timeout\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        # Sort by index to maintain order\n",
    "        sorted_data = sorted(data[\"data\"], key=lambda x: x[\"index\"])\n",
    "        return [item[\"embedding\"] for item in sorted_data]\n",
    "    \n",
    "    def embed(self, text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Embed a single text string.\n",
    "        \n",
    "        Args:\n",
    "            text: Text to embed\n",
    "            \n",
    "        Returns:\n",
    "            Embedding vector as list of floats\n",
    "        \"\"\"\n",
    "        return self._embed_batch([text])[0]\n",
    "    \n",
    "    def embed_many(\n",
    "        self, \n",
    "        texts: List[str], \n",
    "        show_progress: bool = True\n",
    "    ) -> List[EmbeddingResult]:\n",
    "        \"\"\"\n",
    "        Embed multiple texts with automatic batching.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of texts to embed\n",
    "            show_progress: Whether to print progress updates\n",
    "            \n",
    "        Returns:\n",
    "            List of EmbeddingResult objects\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        total_batches = (len(texts) + self.batch_size - 1) // self.batch_size\n",
    "        \n",
    "        for batch_idx, i in enumerate(range(0, len(texts), self.batch_size)):\n",
    "            batch_texts = texts[i:i + self.batch_size]\n",
    "            embeddings = self._embed_batch(batch_texts)\n",
    "            \n",
    "            for j, (text, embedding) in enumerate(zip(batch_texts, embeddings)):\n",
    "                results.append(EmbeddingResult(\n",
    "                    text=text,\n",
    "                    embedding=embedding,\n",
    "                    index=i + j\n",
    "                ))\n",
    "            \n",
    "            if show_progress:\n",
    "                print(f\"\\rProcessed batch {batch_idx + 1}/{total_batches} \"\n",
    "                      f\"({len(results)}/{len(texts)} texts)\", end=\"\")\n",
    "        \n",
    "        if show_progress:\n",
    "            print()  # New line after progress\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def embed_iter(\n",
    "        self, \n",
    "        texts: List[str]\n",
    "    ) -> Iterator[Tuple[int, str, List[float]]]:\n",
    "        \"\"\"\n",
    "        Generator that yields embeddings as they're computed.\n",
    "        Memory-efficient for large datasets.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of texts to embed\n",
    "            \n",
    "        Yields:\n",
    "            Tuples of (index, text, embedding)\n",
    "        \"\"\"\n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch_texts = texts[i:i + self.batch_size]\n",
    "            embeddings = self._embed_batch(batch_texts)\n",
    "            \n",
    "            for j, (text, embedding) in enumerate(zip(batch_texts, embeddings)):\n",
    "                yield (i + j, text, embedding)\n",
    "    \n",
    "    def embed_to_numpy(self, texts: List[str], show_progress: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Embed texts and return as a numpy array.\n",
    "        Ideal for vector database ingestion.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of texts to embed\n",
    "            show_progress: Whether to print progress updates\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of shape (n_texts, embedding_dimension)\n",
    "        \"\"\"\n",
    "        results = self.embed_many(texts, show_progress=show_progress)\n",
    "        return np.array([r.embedding for r in results], dtype=np.float32)\n",
    "    \n",
    "    def similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate cosine similarity between two texts.\n",
    "        \n",
    "        Args:\n",
    "            text1: First text\n",
    "            text2: Second text\n",
    "            \n",
    "        Returns:\n",
    "            Cosine similarity score (0-1)\n",
    "        \"\"\"\n",
    "        embeddings = self._embed_batch([text1, text2])\n",
    "        v1 = np.array(embeddings[0])\n",
    "        v2 = np.array(embeddings[1])\n",
    "        return float(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))\n",
    "\n",
    "\n",
    "# Create a default client instance with model limits\n",
    "client = EmbeddingClient(\n",
    "    base_url=LM_STUDIO_URL,\n",
    "    model=EMBEDDING_MODEL,\n",
    "    context_length=2048,\n",
    "    model_batch_size=1024\n",
    ")\n",
    "\n",
    "print(f\"EmbeddingClient initialized\")\n",
    "print(f\"  Server: {client.base_url}\")\n",
    "print(f\"  Model: {client.model}\")\n",
    "print(f\"  Batch size: {client.batch_size}\")\n",
    "print(f\"  Context length: {client.context_length} tokens\")\n",
    "print(f\"  Model batch size: {client.model_batch_size} tokens\")\n",
    "print(f\"  Embedding dimension: {client.dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5986993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the EmbeddingClient API\n",
    "\n",
    "# Sample documents (simulating content for a vector database)\n",
    "documents = [\n",
    "    \"Python is a high-level programming language known for its readability.\",\n",
    "    \"Machine learning models can learn patterns from large datasets.\",\n",
    "    \"Vector databases store embeddings for semantic search applications.\",\n",
    "    \"LM Studio provides a local server for running language models.\",\n",
    "    \"Neural networks are inspired by biological brain structures.\",\n",
    "    \"Deep learning has revolutionized computer vision and NLP.\",\n",
    "    \"Embeddings represent text as dense numerical vectors.\",\n",
    "    \"Transformers use attention mechanisms to process sequences.\",\n",
    "    \"RAG combines retrieval with generative AI for better responses.\",\n",
    "    \"Local AI solutions offer privacy and data security benefits.\",\n",
    "]\n",
    "\n",
    "# 1. Embed all documents at once (returns EmbeddingResult objects)\n",
    "print(\"1. Embedding documents...\")\n",
    "results = client.embed_many(documents)\n",
    "print(f\"   Generated {len(results)} embeddings\\n\")\n",
    "\n",
    "# 2. Get embeddings as numpy array (ready for vector DB)\n",
    "print(\"2. Getting embeddings as numpy array...\")\n",
    "embeddings_matrix = client.embed_to_numpy(documents)\n",
    "print(f\"   Shape: {embeddings_matrix.shape}\\n\")\n",
    "\n",
    "# 3. Calculate similarity between texts\n",
    "print(\"3. Semantic similarity examples:\")\n",
    "pairs = [\n",
    "    (\"Python is great for AI\", \"Machine learning uses Python often\"),\n",
    "    (\"Python is great for AI\", \"The weather is sunny today\"),\n",
    "    (\"Vector databases\", \"Embedding storage systems\"),\n",
    "]\n",
    "for text1, text2 in pairs:\n",
    "    sim = client.similarity(text1, text2)\n",
    "    print(f\"   '{text1[:30]}...' vs '{text2[:30]}...': {sim:.3f}\")\n",
    "\n",
    "# 4. Find most similar document to a query\n",
    "print(\"\\n4. Semantic search example:\")\n",
    "query = \"How do neural networks learn?\"\n",
    "query_embedding = np.array(client.embed(query))\n",
    "\n",
    "# Calculate similarities\n",
    "similarities = []\n",
    "for i, doc in enumerate(documents):\n",
    "    doc_embedding = embeddings_matrix[i]\n",
    "    sim = np.dot(query_embedding, doc_embedding) / (\n",
    "        np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)\n",
    "    )\n",
    "    similarities.append((sim, doc))\n",
    "\n",
    "# Sort by similarity\n",
    "similarities.sort(reverse=True)\n",
    "print(f\"   Query: '{query}'\")\n",
    "print(f\"   Top 3 matches:\")\n",
    "for sim, doc in similarities[:3]:\n",
    "    print(f\"   {sim:.3f}: {doc[:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cfe813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance test pushing model limits\n",
    "# Context length: 2048 tokens, Batch size: 1024 tokens\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "def generate_text(target_tokens: int) -> str:\n",
    "    \"\"\"Generate random text with approximately target_tokens tokens.\"\"\"\n",
    "    # Conservative estimate: ~4 chars per token\n",
    "    # Use ~3.5 chars to leave safety margin\n",
    "    char_count = int(target_tokens * 3.5)\n",
    "    words = []\n",
    "    while len(' '.join(words)) < char_count:\n",
    "        word_len = random.randint(3, 10)\n",
    "        word = ''.join(random.choices(string.ascii_lowercase, k=word_len))\n",
    "        words.append(word)\n",
    "    return ' '.join(words)[:char_count]\n",
    "\n",
    "def generate_long_text(target_chars: int) -> str:\n",
    "    \"\"\"Generate random text with specific character count for long texts.\"\"\"\n",
    "    words = []\n",
    "    while len(' '.join(words)) < target_chars:\n",
    "        word_len = random.randint(3, 12)\n",
    "        word = ''.join(random.choices(string.ascii_lowercase, k=word_len))\n",
    "        words.append(word)\n",
    "    return ' '.join(words)[:target_chars]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PERFORMANCE TEST: Pushing Model Limits\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Context length limit: {client.context_length} tokens\")\n",
    "print(f\"Batch size limit: {client.model_batch_size} tokens\")\n",
    "print()\n",
    "\n",
    "# Test configurations\n",
    "# Each test varies: (batch_count, tokens_per_text, description)\n",
    "test_configs = [\n",
    "    # Many short texts (maximize parallelism)\n",
    "    (64, 15, \"64 short texts (~15 tokens each)\"),\n",
    "    (128, 7, \"128 very short texts (~7 tokens each)\"),\n",
    "    \n",
    "    # Fewer longer texts (test context handling)\n",
    "    (16, 60, \"16 medium texts (~60 tokens each)\"),\n",
    "    (8, 120, \"8 longer texts (~120 tokens each)\"),\n",
    "    \n",
    "    # Push toward limits (conservative to avoid errors)\n",
    "    (32, 30, \"32 texts √ó 30 tokens (~960 total)\"),\n",
    "    (20, 50, \"20 texts √ó 50 tokens (~1000 total)\"),\n",
    "    \n",
    "    # Single long text (max context utilization)\n",
    "    (1, 500, \"1 long text (~500 tokens)\"),\n",
    "    (2, 400, \"2 long texts (~400 tokens each)\"),\n",
    "    \n",
    "    # Near context limit - single texts\n",
    "    (1, 1500, \"1 text at ~1500 tokens (near limit)\"),\n",
    "    (1, 1800, \"1 text at ~1800 tokens (close to 2048)\"),\n",
    "]\n",
    "\n",
    "results_summary = []\n",
    "\n",
    "for batch_count, tokens_per_text, description in test_configs:\n",
    "    # Generate test texts\n",
    "    texts = [generate_text(tokens_per_text) for _ in range(batch_count)]\n",
    "    \n",
    "    # Estimate total tokens\n",
    "    total_chars = sum(len(t) for t in texts)\n",
    "    est_tokens = total_chars // 4\n",
    "    \n",
    "    # Run the embedding\n",
    "    try:\n",
    "        start_time = time.perf_counter()\n",
    "        embeddings = client._embed_batch(texts)\n",
    "        elapsed = time.perf_counter() - start_time\n",
    "        \n",
    "        throughput = batch_count / elapsed\n",
    "        tokens_per_sec = est_tokens / elapsed\n",
    "        \n",
    "        results_summary.append({\n",
    "            'description': description,\n",
    "            'batch_count': batch_count,\n",
    "            'tokens_per_text': tokens_per_text,\n",
    "            'est_total_tokens': est_tokens,\n",
    "            'elapsed': elapsed,\n",
    "            'throughput': throughput,\n",
    "            'tokens_per_sec': tokens_per_sec,\n",
    "            'status': 'OK'\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úì {description}\")\n",
    "        print(f\"  Chars: {total_chars:,} | Est. tokens: {est_tokens:,}\")\n",
    "        print(f\"  Time: {elapsed*1000:.1f}ms | {throughput:.1f} texts/s | {tokens_per_sec:.0f} tokens/s\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        results_summary.append({\n",
    "            'description': description,\n",
    "            'status': f'FAILED: {str(e)[:50]}'\n",
    "        })\n",
    "        print(f\"‚úó {description}\")\n",
    "        print(f\"  ERROR: {e}\")\n",
    "        print()\n",
    "\n",
    "# =============================================================================\n",
    "# LONG TEXT TESTS (10K-20K characters)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LONG TEXT TESTS: 10K-20K character texts\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Note: These texts will likely be truncated to context_length by the model\")\n",
    "print()\n",
    "\n",
    "long_text_configs = [\n",
    "    (10000, \"10K chars (~2,500 tokens)\"),\n",
    "    (12000, \"12K chars (~3,000 tokens)\"),\n",
    "    (15000, \"15K chars (~3,750 tokens)\"),\n",
    "    (18000, \"18K chars (~4,500 tokens)\"),\n",
    "    (20000, \"20K chars (~5,000 tokens)\"),\n",
    "]\n",
    "\n",
    "for char_count, description in long_text_configs:\n",
    "    # Generate single long text\n",
    "    text = generate_long_text(char_count)\n",
    "    actual_chars = len(text)\n",
    "    est_tokens = actual_chars // 4\n",
    "    \n",
    "    try:\n",
    "        start_time = time.perf_counter()\n",
    "        embeddings = client._embed_batch([text])\n",
    "        elapsed = time.perf_counter() - start_time\n",
    "        \n",
    "        tokens_per_sec = est_tokens / elapsed\n",
    "        \n",
    "        results_summary.append({\n",
    "            'description': description,\n",
    "            'batch_count': 1,\n",
    "            'tokens_per_text': est_tokens,\n",
    "            'est_total_tokens': est_tokens,\n",
    "            'elapsed': elapsed,\n",
    "            'throughput': 1 / elapsed,\n",
    "            'tokens_per_sec': tokens_per_sec,\n",
    "            'status': 'OK',\n",
    "            'long_text': True\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úì {description}\")\n",
    "        print(f\"  Actual chars: {actual_chars:,} | Est. tokens: {est_tokens:,}\")\n",
    "        print(f\"  Time: {elapsed*1000:.1f}ms | {tokens_per_sec:.0f} tokens/s\")\n",
    "        print(f\"  Embedding dim: {len(embeddings[0])}\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        results_summary.append({\n",
    "            'description': description,\n",
    "            'status': f'FAILED',\n",
    "            'error': error_msg[:100]\n",
    "        })\n",
    "        print(f\"‚úó {description}\")\n",
    "        print(f\"  ERROR: {error_msg[:200]}\")\n",
    "        print()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Config':<45} {'Tokens':<8} {'Time':<10} {'Tokens/s':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for r in results_summary:\n",
    "    if r.get('status') == 'OK':\n",
    "        marker = \"üìÑ\" if r.get('long_text') else \"  \"\n",
    "        print(f\"{marker}{r['description']:<43} {r['est_total_tokens']:<8} {r['elapsed']*1000:>6.1f}ms {r['tokens_per_sec']:>8.0f}\")\n",
    "    else:\n",
    "        print(f\"‚ùå{r['description']:<43} {r.get('status', 'FAILED')}\")\n",
    "\n",
    "# Find optimal configuration\n",
    "successful = [r for r in results_summary if r.get('status') == 'OK']\n",
    "if successful:\n",
    "    best = max(successful, key=lambda x: x['tokens_per_sec'])\n",
    "    print()\n",
    "    print(f\"üèÜ Best throughput: {best['description']}\")\n",
    "    print(f\"   {best['tokens_per_sec']:.0f} tokens/second\")\n",
    "    \n",
    "    # Also show best for short vs long texts\n",
    "    short_texts = [r for r in successful if not r.get('long_text')]\n",
    "    long_texts = [r for r in successful if r.get('long_text')]\n",
    "    \n",
    "    if short_texts:\n",
    "        best_short = max(short_texts, key=lambda x: x['tokens_per_sec'])\n",
    "        print(f\"\\nüìä Best for batched short texts: {best_short['description']}\")\n",
    "        print(f\"   {best_short['tokens_per_sec']:.0f} tokens/second\")\n",
    "    \n",
    "    if long_texts:\n",
    "        best_long = max(long_texts, key=lambda x: x['tokens_per_sec'])\n",
    "        print(f\"\\nüìÑ Best for long texts: {best_long['description']}\")\n",
    "        print(f\"   {best_long['tokens_per_sec']:.0f} tokens/second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bb433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# WIKIPEDIA EMBEDDING ESTIMATION\n",
    "# =============================================================================\n",
    "# Estimate time to embed 7M Wikipedia articles with multiple paragraphs each\n",
    "\n",
    "# Wikipedia dataset parameters\n",
    "WIKIPEDIA_ARTICLES = 7_000_000\n",
    "AVG_PARAGRAPHS_PER_ARTICLE = 5  # Typical article has ~5 paragraphs\n",
    "AVG_SENTENCES_PER_PARAGRAPH = 4  # ~4 sentences per paragraph\n",
    "AVG_CHARS_PER_SENTENCE = 120    # Average sentence length\n",
    "\n",
    "# Calculate total fragments\n",
    "TOTAL_PARAGRAPHS = WIKIPEDIA_ARTICLES * AVG_PARAGRAPHS_PER_ARTICLE\n",
    "TOTAL_SENTENCES = TOTAL_PARAGRAPHS * AVG_SENTENCES_PER_PARAGRAPH\n",
    "\n",
    "def format_duration(seconds: float) -> str:\n",
    "    \"\"\"Format seconds into human-readable duration.\"\"\"\n",
    "    if seconds < 60:\n",
    "        return f\"{seconds:.1f} seconds\"\n",
    "    elif seconds < 3600:\n",
    "        return f\"{seconds/60:.1f} minutes\"\n",
    "    elif seconds < 86400:\n",
    "        return f\"{seconds/3600:.1f} hours\"\n",
    "    else:\n",
    "        days = seconds / 86400\n",
    "        return f\"{days:.1f} days\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"WIKIPEDIA EMBEDDING TIME ESTIMATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nDataset assumptions:\")\n",
    "print(f\"  Articles:              {WIKIPEDIA_ARTICLES:,}\")\n",
    "print(f\"  Paragraphs/article:    {AVG_PARAGRAPHS_PER_ARTICLE}\")\n",
    "print(f\"  Sentences/paragraph:   {AVG_SENTENCES_PER_PARAGRAPH}\")\n",
    "print(f\"  Chars/sentence:        {AVG_CHARS_PER_SENTENCE}\")\n",
    "print(f\"\\nTotal fragments to embed:\")\n",
    "print(f\"  Paragraphs:            {TOTAL_PARAGRAPHS:,}\")\n",
    "print(f\"  Sentences:             {TOTAL_SENTENCES:,}\")\n",
    "\n",
    "# Test different fragment sizes to find optimal configuration\n",
    "fragment_configs = [\n",
    "    # (chars_per_fragment, fragments_per_batch, description)\n",
    "    (120, 64, \"Sentence-level (~120 chars, 64/batch)\"),\n",
    "    (120, 128, \"Sentence-level (~120 chars, 128/batch)\"),\n",
    "    (500, 16, \"Paragraph-level (~500 chars, 16/batch)\"),\n",
    "    (500, 32, \"Paragraph-level (~500 chars, 32/batch)\"),\n",
    "    (1000, 8, \"Long paragraph (~1000 chars, 8/batch)\"),\n",
    "    (2000, 4, \"Multi-paragraph (~2000 chars, 4/batch)\"),\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"FRAGMENT THROUGHPUT BENCHMARKS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "fragment_results = []\n",
    "\n",
    "for chars_per_frag, frags_per_batch, description in fragment_configs:\n",
    "    # Generate test fragments\n",
    "    fragments = [generate_long_text(chars_per_frag) for _ in range(frags_per_batch)]\n",
    "    \n",
    "    # Run multiple iterations for stable measurement\n",
    "    iterations = 5\n",
    "    times = []\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        start = time.perf_counter()\n",
    "        client._embed_batch(fragments)\n",
    "        times.append(time.perf_counter() - start)\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    frags_per_sec = frags_per_batch / avg_time\n",
    "    \n",
    "    # Estimate Wikipedia processing time\n",
    "    if chars_per_frag <= 200:  # Sentence-level\n",
    "        total_frags = TOTAL_SENTENCES\n",
    "        frag_type = \"sentences\"\n",
    "    else:  # Paragraph-level\n",
    "        total_frags = TOTAL_PARAGRAPHS\n",
    "        frag_type = \"paragraphs\"\n",
    "    \n",
    "    est_seconds = total_frags / frags_per_sec\n",
    "    \n",
    "    fragment_results.append({\n",
    "        'description': description,\n",
    "        'chars_per_frag': chars_per_frag,\n",
    "        'frags_per_batch': frags_per_batch,\n",
    "        'frags_per_sec': frags_per_sec,\n",
    "        'frag_type': frag_type,\n",
    "        'total_frags': total_frags,\n",
    "        'est_seconds': est_seconds\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n‚úì {description}\")\n",
    "    print(f\"  Batch time: {avg_time*1000:.1f}ms | {frags_per_sec:.1f} fragments/sec\")\n",
    "    print(f\"  Wikipedia ({total_frags:,} {frag_type}): {format_duration(est_seconds)}\")\n",
    "\n",
    "# Summary and recommendations\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"WIKIPEDIA PROCESSING TIME ESTIMATES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n{'Configuration':<45} {'Frags/s':<12} {'Est. Time':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for r in fragment_results:\n",
    "    print(f\"{r['description']:<45} {r['frags_per_sec']:>8.1f}    {format_duration(r['est_seconds']):<15}\")\n",
    "\n",
    "# Find best configuration\n",
    "best_sentence = min([r for r in fragment_results if r['frag_type'] == 'sentences'], \n",
    "                     key=lambda x: x['est_seconds'])\n",
    "best_paragraph = min([r for r in fragment_results if r['frag_type'] == 'paragraphs'], \n",
    "                      key=lambda x: x['est_seconds'])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RECOMMENDATIONS FOR 7M WIKIPEDIA ARTICLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìù SENTENCE-LEVEL EMBEDDINGS ({TOTAL_SENTENCES:,} sentences)\")\n",
    "print(f\"   Best config: {best_sentence['description']}\")\n",
    "print(f\"   Throughput:  {best_sentence['frags_per_sec']:.1f} sentences/second\")\n",
    "print(f\"   Est. time:   {format_duration(best_sentence['est_seconds'])}\")\n",
    "\n",
    "print(f\"\\nüìÑ PARAGRAPH-LEVEL EMBEDDINGS ({TOTAL_PARAGRAPHS:,} paragraphs)\")\n",
    "print(f\"   Best config: {best_paragraph['description']}\")\n",
    "print(f\"   Throughput:  {best_paragraph['frags_per_sec']:.1f} paragraphs/second\")\n",
    "print(f\"   Est. time:   {format_duration(best_paragraph['est_seconds'])}\")\n",
    "\n",
    "# Storage estimate\n",
    "embedding_dim = client.dimension\n",
    "bytes_per_embedding = embedding_dim * 4  # float32\n",
    "\n",
    "print(f\"\\nüíæ STORAGE ESTIMATES (768-dim float32 embeddings)\")\n",
    "print(f\"   Per embedding: {bytes_per_embedding:,} bytes\")\n",
    "print(f\"   Sentence-level: {TOTAL_SENTENCES * bytes_per_embedding / 1e12:.2f} TB\")\n",
    "print(f\"   Paragraph-level: {TOTAL_PARAGRAPHS * bytes_per_embedding / 1e9:.1f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
